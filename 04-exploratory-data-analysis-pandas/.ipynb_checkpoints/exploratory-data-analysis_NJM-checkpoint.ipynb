{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Pandas for Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Define what Pandas is and how it relates to data science.\n",
    "- Manipulate Pandas `DataFrames` and `Series`.\n",
    "- Filter and sort data using Pandas.\n",
    "- Manipulate `DataFrame` columns.\n",
    "- Know how to handle null and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pandas\"></a>\n",
    "\n",
    "## 1. What Is Pandas?\n",
    "\n",
    "Pandas is a Python library that primarily adds two new datatypes to Python: `DataFrame` and `Series`.\n",
    "\n",
    "- A `Series` is a sequence of items, where each item has a unique label (called an `index`).\n",
    "- A `DataFrame` is a table of data. Each row has a unique label (the `row index`), and each column has a unique label (the `column index`).\n",
    "- Note that each column in a `DataFrame` can be considered a `Series` (`Series` index).\n",
    "\n",
    "Behind the scenes, these datatypes use the NumPy (\"Numerical Python\") library. NumPy primarily adds the `ndarray` (n-dimensional array) datatype to Pandas. An `ndarray` is similar to a Python list â€” it stores ordered data. However, it differs in three respects:\n",
    "\n",
    "* Each element has the same datatype (typically fixed-size, e.g., a 32-bit integer).\n",
    "* Elements are stored contiguously (immediately after each other) in memory for fast retrieval.\n",
    "* The total size of an `ndarray` is fixed.\n",
    "\n",
    "Storing `Series` and `DataFrame` data in `ndarray`s makes Pandas faster and uses less memory than standard Python datatypes. Many libraries (such as scikit-learn) accept `ndarray`s as input rather than Pandas datatypes, so we will frequently convert between them.\n",
    "\n",
    "\n",
    "### 1.1 Using Pandas\n",
    "\n",
    "Pandas is frequently used in data science because it offers a large set of commonly used functions, is relatively fast, and has a large community. Because many data science libraries also use NumPy to manipulate data, you can easily transfer data between libraries (as we will often do in this class!).\n",
    "\n",
    "Pandas is a large library that typically takes a lot of practice to learn. It heavily overrides Python operators, resulting in odd-looking syntax. For example, given a `DataFrame` called `cars` which contains a column `mpg`, we might want to view all cars with mpg over 35. To do this, we might write: `cars[cars['mpg'] > 35]`. \n",
    "\n",
    "In standard Python, this would most likely give a syntax error.  \n",
    "\n",
    "Pandas also highly favors certain patterns of use. \n",
    "\n",
    "For example, looping through a `DataFrame` row by row is highly discouraged. \n",
    "\n",
    "Instead, Pandas favors using **vectorized functions** that operate column by column. (This is because each column is stored separately as an `ndarray`, and NumPy is optimized for operating on `ndarray`s.)\n",
    "\n",
    "Do not be discouraged if Pandas feels overwhelming. Gradually, as you use it, you will become familiar with which methods to use and the \"Pandas way\" of thinking about and manipulating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Class Methods and Attributes\n",
    "\n",
    "The Pandas `DataFrame` is a Pandas class object, and therefore comes with a set of attributes (or properties) and methods that can be applied specifically to Pandas ``DataFrames``. \n",
    "\n",
    "To access these, follow the variable name with a dot. For example, given a `DataFrame` called `users`:\n",
    "\n",
    "```\n",
    "- users.index       # accesses the `index` attribute -- note there are no parentheses. attributes are not callable\n",
    "\n",
    "- users.head()      # calls the `head` method (since there are open/closed parentheses)\n",
    "\n",
    "- users.head(10)    # calls the `head` method with parameter `10`, indicating the first 10 rows. this is the same as:\n",
    "\n",
    "- users.head(n=10)  # calls the `head` method, setting the named parameter `n` to have a value of `10`.\n",
    "```\n",
    "\n",
    "Let's try it out by reading in a CSV file and accessing some of its attributes (downloaded from here (Table 32): https://data.gov.uk/dataset/44864962-e4ad-46e6-8f10-71b40126cefb/higher-education-student-data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing ``pandas`` and reading in a CSV file using the ``read_csv`` function. We've also imported ``matplotlib`` for plotting, which we'll do later.\n",
    "\n",
    "We preview the first five rows of the ``DataFrame`` using the ``head`` method.\n",
    "\n",
    "The ``header`` parameter specifies that the column names are in row ``16`` of the underlying CSV file. \n",
    "\n",
    "Notice that ``pandas`` picks out the column names, and numbers the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject of study marker</th>\n",
       "      <th>Subject of study</th>\n",
       "      <th>Level of qualification</th>\n",
       "      <th>Mode of study</th>\n",
       "      <th>Academic Year</th>\n",
       "      <th>Ethnicity marker</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject area</td>\n",
       "      <td>(1) Medicine &amp; dentistry</td>\n",
       "      <td>All</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>2017/18</td>\n",
       "      <td>White</td>\n",
       "      <td>7355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject area</td>\n",
       "      <td>(1) Medicine &amp; dentistry</td>\n",
       "      <td>All</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>2017/18</td>\n",
       "      <td>Black</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject area</td>\n",
       "      <td>(1) Medicine &amp; dentistry</td>\n",
       "      <td>All</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>2017/18</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject area</td>\n",
       "      <td>(1) Medicine &amp; dentistry</td>\n",
       "      <td>All</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>2017/18</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject area</td>\n",
       "      <td>(1) Medicine &amp; dentistry</td>\n",
       "      <td>All</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>2017/18</td>\n",
       "      <td>Other</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject of study marker          Subject of study Level of qualification  \\\n",
       "0            Subject area  (1) Medicine & dentistry                    All   \n",
       "1            Subject area  (1) Medicine & dentistry                    All   \n",
       "2            Subject area  (1) Medicine & dentistry                    All   \n",
       "3            Subject area  (1) Medicine & dentistry                    All   \n",
       "4            Subject area  (1) Medicine & dentistry                    All   \n",
       "\n",
       "  Mode of study Academic Year Ethnicity marker  Number  \n",
       "0     Full-time       2017/18            White    7355  \n",
       "1     Full-time       2017/18            Black     425  \n",
       "2     Full-time       2017/18            Asian    2705  \n",
       "3     Full-time       2017/18            Mixed     495  \n",
       "4     Full-time       2017/18            Other     305  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "university_df = pd.read_csv('./data/Higher_Ed_by_Ethnicity.csv',header=16)\n",
    "university_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the ``index`` attribute shows us how many rows are in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly access the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the ``shape`` attribute is a better way of figuring out how big our dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that our ``DataFrame`` is the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(university_df)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the types of individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, we could convert non-numeric types to numeric types using the ``to_numeric`` function. Note that the ``str.replace()`` function has been used to remove the commas in the ``Travel and Hotel Costs (Base Sales)`` column before it can be converted into a numeric type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df['Number'] = pd.to_numeric(university_df['Number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access all the values in the ``DataFrame`` as a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Selecting and indexing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames have structural similarities with Python-style lists and dictionaries.  \n",
    "\n",
    "In the example below, we select a column of data using the name of the column in a similar manner to how we select a dictionary value with the dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Making friendly columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's easier to manipulate the columns in lower case and unders_core\n",
    "\n",
    "pd_friendly_columns = [col_string.replace(' ', '_') for col_string in university_df.columns]\n",
    "\n",
    "university_df.columns = pd_friendly_columns\n",
    "\n",
    "university_df.columns = map(str.lower, university_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "university_df['academic_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(university_df['academic_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select a single column using this syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "university_df[['academic_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(university_df[['academic_year']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select multiple columns using ``loc``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select multiple columns â€” yet another overload of the DataFrame indexing operator!\n",
    "my_cols = ['academic_year', 'subject_of_study']     # Create a list of column names...\n",
    "university_df[my_cols]                    # ...and use that list to select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or, combine into a single step (this is a Python list inside of the Python index operator!).\n",
    "university_df[['academic_year', 'subject_of_study'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"loc\" locates the values from the first parameter (colon means \"all rows\"), and the column \"Destination\".\n",
    "university_df.loc[:, 'academic_year']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two columns.\n",
    "university_df.loc[:, ['academic_year', 'subject_of_study']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a range of columns â€” unlike Python ranges, Pandas index ranges INCLUDE the final column in the range.\n",
    "university_df.loc[:, 'academic_year' : 'number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"loc\" can also filter rows by \"name\" (the index).\n",
    "# Row 0, all columns\n",
    "university_df.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0/1/2, all columns\n",
    "university_df.loc[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0/1/2, range of columns\n",
    "university_df.loc[0:2, 'academic_year' : 'number'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"iloc\" to filter rows and select columns by integer position.\n",
    "# (Remember that rows/columns use indices, so \"iloc\" lets you refer to indices via their index rather than value!)\n",
    "# All rows, columns in position 0/3  \n",
    "university_df.iloc[:, [0, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows, columns in position 0/1/2/3\n",
    "# Note here it is NOT INCLUDING 4 because this is an integer range, not a Pandas index range!\n",
    "university_df.iloc[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows in position 0/1/2, all columns\n",
    "university_df.iloc[0:3, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Summarising the data\n",
    "\n",
    "Pandas has a bunch of built-in methods to quickly summarize your data and provide you with a quick general understanding. The ``describe`` method gives summary statistics for the numeric columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to get summary statistics for all columns, including non-numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get summaries for individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df['academic_year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``value_counts`` method gives a count for each unique value in a DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "university_df['subject_of_study'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ``hist`` method gives a visual representation of value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df['number'].hist(bins=100,range=[0, 2500],figsize=(16,6),color='red',alpha=0.6);\n",
    "plt.xlabel('Number of students');\n",
    "plt.ylabel('Count');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df['academic_year'].value_counts().plot(kind='bar',figsize=(16,6),alpha=0.6)  \n",
    "plt.xlabel('Academic Year');\n",
    "plt.ylabel('Count');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering-and-sorting\"></a>\n",
    "## Filtering and Sorting Data\n",
    "\n",
    "We can use simple operator comparisons on columns to extract relevant or drop irrelevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a Series of Booleansâ€¦\n",
    "# In Pandas, this comparison is performed element-wise on each row of data.\n",
    "\n",
    "number_filter = university_df['number'] > 3000\n",
    "number_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€¦and use that Series to filter rows.\n",
    "# In Pandas, indexing a DataFrame by a Series of Booleans only selects rows that are True in the Boolean.\n",
    "\n",
    "university_df[number_filter].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter on strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicine_filter = university_df['subject_of_study'] == '(1) Medicine & dentistry'\n",
    "medicine_df = university_df[medicine_filter]\n",
    "medicine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ``str.contains()`` method to find all rows that contain a particular string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_filter = university_df['subject_of_study'].str.contains('science')\n",
    "science_df = university_df[science_filter]\n",
    "science_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine this into a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_df = university_df[university_df['subject_of_study'].str.contains('science')]\n",
    "science_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine multiple logical tests into a single filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_time_medicine_filter = (university_df['subject_of_study'] == \n",
    "                             \n",
    "                             '(1) Medicine & dentistry') & (university_df['mode_of_study'] == 'Part-time')\n",
    "    \n",
    "    \n",
    "part_time_medicine_df = university_df[part_time_medicine_filter]\n",
    "part_time_medicine_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more efficient alternative to using lots of 'OR' statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "university_df[university_df['subject_of_study'].isin(['(1) Medicine & dentistry', '(H1) General engineering'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Setting values in Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one column from the filtered results. 'at method' to access specific column, with a label and renaming it.\n",
    "#https://pandas.pydata.org/pandas-docs/version/0.25/reference/api/pandas.DataFrame.at.html\n",
    "university_df.at[2, 'subject_of_study'] = 'test_value'\n",
    "university_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to sort the rows in a DataFrame by one column in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "university_df.sort_values(by='number',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming, Adding, and Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column labels\n",
    "print(university_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename one or more columns in a single output using value mapping.\n",
    "university_df.rename(columns={'subject_of_study':'subject', 'academic_year':'the_blessed_year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why haven't the column names changed when we try to access them?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename one or more columns in the original DataFrame.\n",
    "university_df.rename(columns={'subject_of_study':'subject', 'academic_year':'year'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = ['subject_marker','subject','level','study_mode',\n",
    "                 'blessed_year', 'ethnicity','number']\n",
    "\n",
    "university_df.columns = new_col_names\n",
    "\n",
    "university_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Column Operations\n",
    "\n",
    "Rather than having to reference indexes and create for loops to do column-wise operations, Pandas is smart and knows that when we add columns together we want to add the values in each row together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column as a function of existing columns.\n",
    "university_df['number_thousands'] = university_df['number']/1000\n",
    "university_df['subject'] = university_df['subject'].str.lower()\n",
    "\n",
    "university_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df.drop(columns=['subject_marker'],inplace=True)\n",
    "university_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Sometimes, values will be missing from the source data or as a byproduct of manipulations. It is very important to detect missing data. Missing data can:\n",
    "\n",
    "- Make the entire row ineligible to be training data for a model.\n",
    "- Hint at data-collection errors.\n",
    "- Indicate improper conversion or manipulation.\n",
    "- Actually not be missing â€” it sometimes means \"zero,\" \"false,\" \"not applicable,\" or \"entered an empty string.\"\n",
    "\n",
    "For example, a `.csv` file might have a missing value in some data fields:\n",
    "\n",
    "```\n",
    "tool_name,material,cost\n",
    "hammer,wood,8\n",
    "chainsaw,,\n",
    "wrench,metal,5\n",
    "```\n",
    "\n",
    "When this data is imported, \"null\" values will be stored in the second row (in the \"material\" and \"cost\" columns).\n",
    "\n",
    "In Pandas, a \"null\" value is either `None` or `np.NaN` (Not a Number). Many fixed-size numeric datatypes (such as integers) do not have a way of representing `np.NaN`. So, numeric columns will be promoted to floating-point datatypes that do support it. For example, when importing the `.csv` file above:\n",
    "\n",
    "**For the second row:** `None` will be stored in the \"material\" column and `np.NaN` will be stored in the \"cost\" column. The entire \"cost\" column (stored as a single `ndarray`) must be stored as floating-point values to accommodate the `np.NaN`, even though an integer `8` is in the first row.\n",
    "\n",
    "Let's read in another dataset ('Expenses Jan to Mar 19' data downloaded from here: https://data.gov.uk/dataset/091af8ee-95db-4336-9902-42ee998be323/senior-officials-expenses-travel-and-hospitality-in-dwp) to understand missing values.\n",
    "\n",
    "You'll notice that a lot of cells contain ``NaN`` or 'not a number'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dwp_expenses_df = pd.read_csv('./data/dwp-senior-officials-expenses-jan-mar-2019.csv',encoding='latin1')\n",
    "dwp_expenses_df.head()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the type of one of these ``NaN`` elements, we'll see it's still 'float'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwp_expenses_df.iloc[0,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dwp_expenses_df.iloc[0,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show rows where the Accommodation/Meals column is not NaN\n",
    "dwp_expenses_df[dwp_expenses_df['Accomodation/Meals'].notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values are usually excluded in calculations by default.\n",
    "dwp_expenses_df['Accomodation/Meals'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Includes missing values\n",
    "dwp_expenses_df['Accomodation/Meals'].value_counts(dropna=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to drop rows containing ``NaN`` values, or fill in ``NaN`` values with a string, float or other element of our choice. \n",
    "\n",
    "Be careful when doing either of these things; you could end up unintentionally removing rows, or filling in values that don't make sense or aren't accurate.\n",
    "\n",
    "In this case, it would be important to clarify whether a ``NaN`` value in a particular column means the amount is zero, or whether it means the amount is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwp_expenses_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by converting the numeric columns to numeric types, stripping away the Â£ sign before the conversion\n",
    "\n",
    "dwp_expenses_df['Accomodation/Meals'] = pd.to_numeric(dwp_expenses_df['Accomodation/Meals'].str.replace('Â£',''),errors='coerce')\n",
    "dwp_expenses_df['Total cost, including all visas, accommodation, travel, meals etc. (Â£)'] = pd.to_numeric(dwp_expenses_df['Total cost, including all visas, accommodation, travel, meals etc. (Â£)'].str.replace('Â£',''),errors='coerce')\n",
    "dwp_expenses_df['Other (including hospitality given)'] = pd.to_numeric(dwp_expenses_df['Other (including hospitality given)'].str.replace('Â£',''),errors='coerce')\n",
    "dwp_expenses_df['Total Cost of Use of Official Secure Car'] = pd.to_numeric(dwp_expenses_df['Total Cost of Use of Official Secure Car'].str.replace('Â£',''),errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row if ANY values are missing from any column â€” can be dangerous!\n",
    "#dwp_expenses_df.dropna()\n",
    "\n",
    "# Drop a row only if ALL values are missing.\n",
    "#dwp_expenses_df.dropna(how='all')\n",
    "\n",
    "# Fill in missing values with 0 â€” this is dangerous to do without manually verifying them!\n",
    "dwp_expenses_df['Accomodation/Meals'].fillna(value=0,inplace=True)\n",
    "dwp_expenses_df['Total cost, including all visas, accommodation, travel, meals etc. (Â£)'].fillna(value=0,inplace=True)\n",
    "dwp_expenses_df['Other (including hospitality given)'].fillna(value=0,inplace=True)\n",
    "dwp_expenses_df['Total Cost of Use of Official Secure Car'].fillna(value=0,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwp_expenses_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Pandas axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=0 sums across rows\n",
    "dwp_expenses_df[['Accomodation/Meals', 'Other (including hospitality given)']].sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# axis=1 sums across columns (doesn't always lead to results that make sense)\n",
    "dwp_expenses_df[['Accomodation/Meals', 'Other (including hospitality given)']].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-apply-combine\"></a>\n",
    "### Split-Apply-Combine\n",
    "\n",
    "Split-apply-combine is a pattern for analyzing data. Suppose we want to find mean travel costs per person. Then:\n",
    "\n",
    "- **Split:** We group data by person.\n",
    "- **Apply:** For each group, we apply the `sum()` function to find the mean travel cost.\n",
    "- **Combine:** We now combine the names with the `sum()`s to produce a summary of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each official, calculate the mean trip duration.\n",
    "dwp_expenses_df.groupby('Name (Firstname, Surname)')['Duration of Visit (Days)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each official, calculate the mean of all numeric columns.\n",
    "dwp_expenses_df.groupby('Name (Firstname, Surname)').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each official, describe total costs.\n",
    "dwp_expenses_df.groupby('Name (Firstname, Surname)')['Total cost, including all visas, accommodation, travel, meals etc. (Â£)'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similar, but outputs a DataFrame and can be customized â€” \"agg\" allows you to aggregate results of Series functions\n",
    "dwp_expenses_df.groupby('Name (Firstname, Surname)').agg(['count', 'mean', 'min', 'max'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "We'll be working with Twitter's election integrity dataset from October 2018, which consists of ~3million tweets from accounts suspected to be associated with overseas bot accounts.\n",
    "\n",
    "Some of the code in these exercises is boilerplated (i.e. written for you), with gaps for you to fill in. Instructions are provided in the comments where this is the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Read in the data\n",
    "\n",
    "Visit this URL, and enter your email address to access information about the datasets. Read a bit about which datasets are available: https://about.twitter.com/en_gb/values/elections-integrity.html#data\n",
    "\n",
    "In particular, read the ``Readme`` file to understand the variables contained in each dataset: \n",
    "\n",
    "\n",
    "\n",
    "We'll be downloading the tweets associated with the **Iran (October 2018) â€“ 770 accounts** dataset. \n",
    "\n",
    "To do this, you should:\n",
    "\n",
    "* Click on this URL, and download the resulting ZIP file to your computer.\n",
    "\n",
    "\n",
    "\n",
    "* Unzip the contents of the file to the ``data`` directory inside the same directory as this notebook. \n",
    "\n",
    "The result should be a file called ``iranian_tweets_csv_hashed.csv`` in location ``./data/`` relative to this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first 10 rows of the dataset using ``head``. What does each row correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``value_counts()`` to get summary counts for the language of the tweets and the location of the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``describe`` to get a summary of the numeric columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Filter out non-UK based accounts and unneccessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data so that only tweets where ``user_reported_location`` is ``United Kingdom`` are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = tweet_df[tweet_df['user_reported_location']=='United Kingdom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the filter has worked by running this next cell of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df[tweet_df['user_reported_location']=='United Kingdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_df[['tweet_text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the following columns:\n",
    "\n",
    "``tweetid,\n",
    "follower_count,\n",
    "user_screen_name,\n",
    "following_count,\n",
    "account_creation_date,\n",
    "tweet_text,\n",
    "tweet_time,\n",
    "like_count,\n",
    "retweet_count``\n",
    "\n",
    "\n",
    "It's more efficient to do this by selecting columns using ``my_df[['col1','col2','col3']]`` notation rather than using ``my_df.drop(columns=[])`` since we want to drop many more columns than we want to keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Which account was purportedly tweeting from the UK?\n",
    "\n",
    "Use ``value_counts()`` on the ``user_screen_name`` column to see how many accounts were claiming to tweet from the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just one user! Let's look at when they tweet, and what they tweet about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Convert columns to the pandas ``datetime`` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``pd.to_datetime()`` function to convert the ``account_creation_date`` and ``tweet_time`` columns into the type ``datetime``. This is a type in ``pandas`` that allows dates to be treated like timestamps, so we can search by date, sort in chronological order etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``dtypes`` to confirm that these columns are now ``datetime`` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) When was this account tweeting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``hist()`` method to get a quick visualisation of the distribution of ``tweet_time``s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Did the account get many likes or retweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``value_counts()`` and ``describe()`` on the ``like_count`` and ``retweet_count`` columns to gauge how successful you think this account was at propagating information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) What was this account tweeting about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to explore the contents of each tweet using some basic word count methods. This is much less advanced than the more involved natural language processing methods we'll be using later on in the course, but is a good start. \n",
    "\n",
    "Let's begin by using the ``str.lower()`` method to convert the ``tweet_text`` column to lowercase so we don't have to worry about case sensitivity, and also using ``str.replace()`` together with a **regular expression** to catch **all puncuation marks** and replace them with an empty string; this is the same thing as stripping out all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['tweet_text'] = tweet_df['tweet_text'].str.lower().str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the replacement has worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_df['tweet_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use filtering and ``.str.contains()`` to find out what **percentage** of the tweets mention the following terms (remember we've converted everything to lowercase so your search terms need to be lowercase as well):\n",
    "\n",
    "* obama\n",
    "* brexit\n",
    "* trump\n",
    "* syria\n",
    "* iran\n",
    "* uk \n",
    "* russia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "### Summary\n",
    "\n",
    "Believe it or not, we've only barely touched the surface of everything that Pandas offers. Don't worry if you don't remember most of it â€” for now, just knowing what exists is key. Remember that the more you use Pandas to manipulate data, the more of these functions you will take interest in, look up, and remember.\n",
    "\n",
    "In this notebook, the most important things to familiarize yourself with are the basics:\n",
    "- Manipulating `DataFrames` and `Series`\n",
    "- Filtering columns and rows\n",
    "- Handling missing values\n",
    "- Split-apply-combine (this one takes some practice!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
