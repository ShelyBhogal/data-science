{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Natural Language Processing (NLP) Review Lab\n",
    "\n",
    "_Authors: Joseph Nelson (DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note: This lab is intended to be done as a walkthrough with the instructor.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker, [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky, Kevin Markham's Data School Curriculum*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"corpus\" = collection of documents\n",
    "- \"corpora\" = plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '../data/yelp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7623</td>\n",
       "      <td>5</td>\n",
       "      <td>I really don't like these types of Mexican foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4547</td>\n",
       "      <td>2</td>\n",
       "      <td>This was real dissapointment because I really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>3</td>\n",
       "      <td>Sacks is a great little place.  One issue and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>2</td>\n",
       "      <td>To even admit to serving tex-mex is the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4686</td>\n",
       "      <td>1</td>\n",
       "      <td>UPDATE: This location is closed. Boo!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                                               text\n",
       "7623      5  I really don't like these types of Mexican foo...\n",
       "4547      2  This was real dissapointment because I really ...\n",
       "398       3  Sacks is a great little place.  One issue and ...\n",
       "2940      2  To even admit to serving tex-mex is the first ...\n",
       "4686      1              UPDATE: This location is closed. Boo!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp[['stars', 'text']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10,000 reviews, each with 10 characteristics\n",
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Being a creature of habit anytime I want good sushi I go to Tokyo Lobby.  Well, my group wanted to branch out and try something new so we decided on Sakana. Not a fan.  And what's shocking to me is this place was packed!  The restaurant opens at 5:30 on Saturday and we arrived at around 5:45 and were lucky to get the last open table.  I don't get it...\\r\\n\\r\\nMessy rolls that all tasted the same.  We ordered the tootsie roll and the crunch roll, both tasted similar, except of course for the crunchy captain crunch on top.  Just a mushy mess, that was hard to eat.  Bland tempura.  No bueno.  I did, however, have a very good tuna poke salad, but I would not go back just for that. \\r\\n\\r\\nIf you want good sushi on the west side, or the entire valley for that matter, say no to Sakana and yes to Tokyo Lobby.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.loc[9005, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.loc[9005, 'stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Subset the reviews to best and worst.\n",
    "\n",
    "- Select only 5-star and 1-star reviews.\n",
    "- The text will be the features, the stars will be the target.\n",
    "- Create a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4241</td>\n",
       "      <td>d_8bMNQd0mesbEUeq1U2kQ</td>\n",
       "      <td>2012-01-16</td>\n",
       "      <td>5jqJpDe8P22HtS4GH4kZ_A</td>\n",
       "      <td>5</td>\n",
       "      <td>This is by far my favorite Indian restaurant i...</td>\n",
       "      <td>review</td>\n",
       "      <td>U1VhMJAKHaTqCSIp9RoRcg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8611</td>\n",
       "      <td>ncxBZxetREZ_jCma0c7mHA</td>\n",
       "      <td>2011-05-14</td>\n",
       "      <td>18bXhCOgwf_-TnvRSr8zWA</td>\n",
       "      <td>1</td>\n",
       "      <td>The wash ain't bad....but the process to get i...</td>\n",
       "      <td>review</td>\n",
       "      <td>OoyMyBD0a-QmEJrFzw78Fw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5042</td>\n",
       "      <td>9BJ5h9X1krpXFjKj0a6wbg</td>\n",
       "      <td>2008-01-20</td>\n",
       "      <td>vgjSeoz6mHX5wVh6MH09sQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Love it!\\r\\nThe food is VERY spicy though.  Lu...</td>\n",
       "      <td>review</td>\n",
       "      <td>PShy2RYNadDUhJf4ErOJ7w</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9696</td>\n",
       "      <td>NA3tQYxR6Fq5O8nV6u41Tw</td>\n",
       "      <td>2011-10-04</td>\n",
       "      <td>0XQNmyxaBbZ_2M9AGA8RrQ</td>\n",
       "      <td>5</td>\n",
       "      <td>My favorite place in the world to eat! Great s...</td>\n",
       "      <td>review</td>\n",
       "      <td>JRkqD8JvtQATNTTv6UI7RA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5013</td>\n",
       "      <td>N80E9zoWEpHhi4kH2eKAsw</td>\n",
       "      <td>2010-08-22</td>\n",
       "      <td>KH1QxaALC-vm1LvKPd95UQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Those of us \"foodies\" know that if the company...</td>\n",
       "      <td>review</td>\n",
       "      <td>NLDDaat42UQXQCOpU4e2TA</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5054</td>\n",
       "      <td>e0Or6HYHL03y7IHl0itIOw</td>\n",
       "      <td>2011-02-24</td>\n",
       "      <td>ZAU_99yxKqM_EEPjcDCqIg</td>\n",
       "      <td>5</td>\n",
       "      <td>All I have to say is wow.  The auto picks and ...</td>\n",
       "      <td>review</td>\n",
       "      <td>wUCRCqCcRFAvX0e17_6odA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4176</td>\n",
       "      <td>NmtZuT8p4vNk259dvozbvg</td>\n",
       "      <td>2012-10-09</td>\n",
       "      <td>H8W9aUhwqWi__d4BuaDkyA</td>\n",
       "      <td>5</td>\n",
       "      <td>Kaley helped me get the best room in Mesa AZ, ...</td>\n",
       "      <td>review</td>\n",
       "      <td>jdPaIEMrUpzR4-dC-o5zSw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9110</td>\n",
       "      <td>t0NencbvVVlH6mcRlNTPcg</td>\n",
       "      <td>2010-12-23</td>\n",
       "      <td>ndSwfewtSaTTjXNm2wR71Q</td>\n",
       "      <td>1</td>\n",
       "      <td>I was excited to go to the lululemon athletica...</td>\n",
       "      <td>review</td>\n",
       "      <td>jwSTtW_q8PULge2dK1t_Lg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>3H2ttTM2aSIaZ6FTjHwDQQ</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>6wih8fh9hGHRCc4tk6U3ew</td>\n",
       "      <td>5</td>\n",
       "      <td>I have only gotten the cafe sua da (iced coffe...</td>\n",
       "      <td>review</td>\n",
       "      <td>UhryFhGe1tqdbBzAhdooTQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8327</td>\n",
       "      <td>rQ4z0EStSZE4acgkne6Hmg</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>OKh2j7gfxprNY_BcKSOp5w</td>\n",
       "      <td>5</td>\n",
       "      <td>Tuck Shop is definitely one of my favorite res...</td>\n",
       "      <td>review</td>\n",
       "      <td>JTwQpwYexzuEkYY53XHdVQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "4241  d_8bMNQd0mesbEUeq1U2kQ  2012-01-16  5jqJpDe8P22HtS4GH4kZ_A      5   \n",
       "8611  ncxBZxetREZ_jCma0c7mHA  2011-05-14  18bXhCOgwf_-TnvRSr8zWA      1   \n",
       "5042  9BJ5h9X1krpXFjKj0a6wbg  2008-01-20  vgjSeoz6mHX5wVh6MH09sQ      5   \n",
       "9696  NA3tQYxR6Fq5O8nV6u41Tw  2011-10-04  0XQNmyxaBbZ_2M9AGA8RrQ      5   \n",
       "5013  N80E9zoWEpHhi4kH2eKAsw  2010-08-22  KH1QxaALC-vm1LvKPd95UQ      5   \n",
       "5054  e0Or6HYHL03y7IHl0itIOw  2011-02-24  ZAU_99yxKqM_EEPjcDCqIg      5   \n",
       "4176  NmtZuT8p4vNk259dvozbvg  2012-10-09  H8W9aUhwqWi__d4BuaDkyA      5   \n",
       "9110  t0NencbvVVlH6mcRlNTPcg  2010-12-23  ndSwfewtSaTTjXNm2wR71Q      1   \n",
       "178   3H2ttTM2aSIaZ6FTjHwDQQ  2012-11-29  6wih8fh9hGHRCc4tk6U3ew      5   \n",
       "8327  rQ4z0EStSZE4acgkne6Hmg  2012-03-12  OKh2j7gfxprNY_BcKSOp5w      5   \n",
       "\n",
       "                                                   text    type  \\\n",
       "4241  This is by far my favorite Indian restaurant i...  review   \n",
       "8611  The wash ain't bad....but the process to get i...  review   \n",
       "5042  Love it!\\r\\nThe food is VERY spicy though.  Lu...  review   \n",
       "9696  My favorite place in the world to eat! Great s...  review   \n",
       "5013  Those of us \"foodies\" know that if the company...  review   \n",
       "5054  All I have to say is wow.  The auto picks and ...  review   \n",
       "4176  Kaley helped me get the best room in Mesa AZ, ...  review   \n",
       "9110  I was excited to go to the lululemon athletica...  review   \n",
       "178   I have only gotten the cafe sua da (iced coffe...  review   \n",
       "8327  Tuck Shop is definitely one of my favorite res...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  \n",
       "4241  U1VhMJAKHaTqCSIp9RoRcg     1       0      0  \n",
       "8611  OoyMyBD0a-QmEJrFzw78Fw     0       1      0  \n",
       "5042  PShy2RYNadDUhJf4ErOJ7w     1       0      0  \n",
       "9696  JRkqD8JvtQATNTTv6UI7RA     0       0      0  \n",
       "5013  NLDDaat42UQXQCOpU4e2TA     3       3      2  \n",
       "5054  wUCRCqCcRFAvX0e17_6odA     1       1      0  \n",
       "4176  jdPaIEMrUpzR4-dC-o5zSw     0       0      0  \n",
       "9110  jwSTtW_q8PULge2dK1t_Lg     0       3      0  \n",
       "178   UhryFhGe1tqdbBzAhdooTQ     0       0      0  \n",
       "8327  JTwQpwYexzuEkYY53XHdVQ     0       0      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_reviews = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "yelp_reviews.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4086, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4086 reviews\n",
    "yelp_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    3337\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Majority class is 5 stars; approx 82%\n",
    "yelp_reviews['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_reviews.text\n",
    "y = yelp_reviews.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the review dataset into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Use CountVectorizer with stop words to convert the training and testing text data - set up function to vectorize and validate\n",
    "\n",
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "- **lowercase:** boolean, True by default\n",
    "    - Convert all characters to lowercase before tokenizing.\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_test(vect):\n",
    "    X_train_vect = vect.fit_transform(X_train)\n",
    "    print(('Features: ', X_train_vect.shape[1]))\n",
    "    \n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_vect, y_train)\n",
    "    \n",
    "    y_pred = nb.predict(X_test_vect)\n",
    "    print(('F1-score: ', metrics.f1_score(y_test, y_pred)))\n",
    "    print(('Accuracy: ', metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16711)\n",
      "('F1-score: ', 0.721407624633431)\n",
      "('Accuracy: ', 0.9070450097847358)\n"
     ]
    }
   ],
   "source": [
    "vectorise_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predict the star rating with the new features from CountVectorizer with Logistic Regression.\n",
    "\n",
    "Validate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "count_train = vect.fit_transform(X_train)\n",
    "count_test = vect.transform(X_test)\n",
    "\n",
    "logreg.fit(count_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890410958904109\n",
      "0.9246575342465754\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression yields better results than MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Recreate your features with CountVectorizer removing stopwords.\n",
    "\n",
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16712)\n",
      "('F1-score: ', 0.7235294117647059)\n",
      "('Accuracy: ', 0.9080234833659491)\n"
     ]
    }
   ],
   "source": [
    "vectorise_test(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Validate your model using the features with stopwords removed using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgrg = LogisticRegression(max_iter=1000)\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_test_vect = vect.transform(X_test)\n",
    "\n",
    "lgrg.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgrg.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890410958904109\n",
      "0.9246575342465754\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(y_test, pred))\n",
    "print(metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results haven't changed whether you keep or remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Shrink the maximum number of features and re-test the model.\n",
    "\n",
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = CountVectorizer(stop_words='english', max_features=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 15000)\n",
      "('F1-score: ', 0.7377521613832853)\n",
      "('Accuracy: ', 0.910958904109589)\n"
     ]
    }
   ],
   "source": [
    "vectorise_test(count_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score improves slightly, accuracy stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Change the minimum document frequency for terms and test the model's performance.\n",
    "\n",
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', min_df=3, max_features=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 6036)\n",
      "('F1-score: ', 0.7631578947368421)\n",
      "('Accuracy: ', 0.9119373776908023)\n"
     ]
    }
   ],
   "source": [
    "vectorise_test(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score improves again, accuracy slighty improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Introduction to TextBlob\n",
    "\n",
    "TextBlob: \"Simplified Text Processing\"\n",
    "\n",
    "### 5.1 Use `TextBlob` to convert the text in the first review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\r\n",
      "\r\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\r\n",
      "\r\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\r\n",
      "\r\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(yelp.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shmel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = TextBlob(yelp.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 List the words in the `TextBlob` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 List the sentences in the `TextBlob` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Stemming and Lemmatization\n",
    "\n",
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Initialize the `SnowballStemmer` and stem the words in the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# See \"amazing\" becomes \"amaz\"\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Use the built-in `lemmatize` function on the words of the first review (parsed by `TextBlob`)\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shmel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'wa', 'excellent', 'The', 'weather', 'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'and', 'it', 'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'piece', 'of', 'their', 'griddled', 'bread', 'with', 'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# See \"was\" becomes \"wa\"\n",
    "nltk.download('wordnet')\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Write a function that uses `TextBlob` and `lemmatize` to lemmatize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_text(text):\n",
    "    text = str(text).lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quiessence',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'put',\n",
       " 'beautiful',\n",
       " 'full',\n",
       " 'window',\n",
       " 'and',\n",
       " 'earthy',\n",
       " 'wooden',\n",
       " 'wall',\n",
       " 'give',\n",
       " 'a',\n",
       " 'feeling',\n",
       " 'of',\n",
       " 'warmth',\n",
       " 'inside',\n",
       " 'this',\n",
       " 'restaurant',\n",
       " 'perched',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'a',\n",
       " 'farm',\n",
       " 'the',\n",
       " 'restaurant',\n",
       " 'seemed',\n",
       " 'fairly',\n",
       " 'full',\n",
       " 'even',\n",
       " 'on',\n",
       " 'a',\n",
       " 'tuesday',\n",
       " 'evening',\n",
       " 'we',\n",
       " 'had',\n",
       " 'secured',\n",
       " 'reservation',\n",
       " 'just',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'day',\n",
       " 'before',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'i',\n",
       " 'had',\n",
       " 'sampled',\n",
       " 'sandwich',\n",
       " 'at',\n",
       " 'the',\n",
       " 'farm',\n",
       " 'kitchen',\n",
       " 'earlier',\n",
       " 'that',\n",
       " 'week',\n",
       " 'and',\n",
       " 'were',\n",
       " 'impressed',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'at',\n",
       " 'the',\n",
       " 'restaurant',\n",
       " 'the',\n",
       " 'crisp',\n",
       " 'fresh',\n",
       " 'veggie',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'disappoint',\n",
       " 'we',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'salad',\n",
       " 'with',\n",
       " 'orange',\n",
       " 'and',\n",
       " 'grapefruit',\n",
       " 'slice',\n",
       " 'and',\n",
       " 'the',\n",
       " 'crudites',\n",
       " 'to',\n",
       " 'start',\n",
       " 'both',\n",
       " 'were',\n",
       " 'very',\n",
       " 'good',\n",
       " 'i',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'even',\n",
       " 'know',\n",
       " 'how',\n",
       " 'much',\n",
       " 'i',\n",
       " 'liked',\n",
       " 'raw',\n",
       " 'radish',\n",
       " 'and',\n",
       " 'turnip',\n",
       " 'until',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'them',\n",
       " 'with',\n",
       " 'their',\n",
       " 'pesto',\n",
       " 'and',\n",
       " 'aioli',\n",
       " 'sauce',\n",
       " 'for',\n",
       " 'entree',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'lamb',\n",
       " 'and',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'pork',\n",
       " 'shoulder',\n",
       " 'service',\n",
       " 'started',\n",
       " 'out',\n",
       " 'very',\n",
       " 'good',\n",
       " 'but',\n",
       " 'trailed',\n",
       " 'off',\n",
       " 'quickly',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'our',\n",
       " 'food',\n",
       " 'took',\n",
       " 'a',\n",
       " 'very',\n",
       " 'long',\n",
       " 'time',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'seated',\n",
       " 'after',\n",
       " 'u',\n",
       " 'received',\n",
       " 'and',\n",
       " 'finished',\n",
       " 'their',\n",
       " 'entree',\n",
       " 'before',\n",
       " 'we',\n",
       " 'received',\n",
       " 'our',\n",
       " \"'s\",\n",
       " 'and',\n",
       " 'no',\n",
       " 'one',\n",
       " 'bothered',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'until',\n",
       " 'the',\n",
       " 'maitre',\n",
       " \"'d\",\n",
       " 'apologized',\n",
       " 'almost',\n",
       " '45',\n",
       " 'minute',\n",
       " 'later',\n",
       " 'apparently',\n",
       " 'the',\n",
       " 'chef',\n",
       " 'wa',\n",
       " 'unhappy',\n",
       " 'with',\n",
       " 'the',\n",
       " 'sauce',\n",
       " 'on',\n",
       " 'my',\n",
       " 'entree',\n",
       " 'so',\n",
       " 'he',\n",
       " 'started',\n",
       " 'anew',\n",
       " 'this',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'really',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'but',\n",
       " 'they',\n",
       " 'should',\n",
       " 'have',\n",
       " 'communicated',\n",
       " 'this',\n",
       " 'to',\n",
       " 'u',\n",
       " 'earlier',\n",
       " 'for',\n",
       " 'our',\n",
       " 'trouble',\n",
       " 'they',\n",
       " 'comped',\n",
       " 'me',\n",
       " 'the',\n",
       " 'glass',\n",
       " 'of',\n",
       " 'wine',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'but',\n",
       " 'they',\n",
       " 'forgot',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'out',\n",
       " 'with',\n",
       " 'my',\n",
       " 'entree',\n",
       " 'a',\n",
       " 'i',\n",
       " 'had',\n",
       " 'requested',\n",
       " 'also',\n",
       " 'they',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'offer',\n",
       " 'u',\n",
       " 'bread',\n",
       " 'but',\n",
       " 'i',\n",
       " 'will',\n",
       " 'echo',\n",
       " 'the',\n",
       " 'lady',\n",
       " 'who',\n",
       " 'whispered',\n",
       " 'this',\n",
       " 'to',\n",
       " 'u',\n",
       " 'on',\n",
       " 'her',\n",
       " 'way',\n",
       " 'out',\n",
       " 'ask',\n",
       " 'for',\n",
       " 'the',\n",
       " 'bread',\n",
       " 'we',\n",
       " 'received',\n",
       " 'warm',\n",
       " 'foccacia',\n",
       " 'apple',\n",
       " 'walnut',\n",
       " 'and',\n",
       " 'pomegranate',\n",
       " 'slice',\n",
       " 'of',\n",
       " 'wonder',\n",
       " 'with',\n",
       " 'honey',\n",
       " 'and',\n",
       " 'butter',\n",
       " 'yum',\n",
       " 'the',\n",
       " 'entree',\n",
       " 'were',\n",
       " 'both',\n",
       " 'solid',\n",
       " 'but',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'quite',\n",
       " 'live',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'innovation',\n",
       " 'and',\n",
       " 'freshness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'vegetable',\n",
       " 'my',\n",
       " 'lamb',\n",
       " \"'s\",\n",
       " 'sauce',\n",
       " 'wa',\n",
       " 'delicious',\n",
       " 'but',\n",
       " 'the',\n",
       " 'meat',\n",
       " 'wa',\n",
       " 'tough',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'vegetarian',\n",
       " 'entree',\n",
       " 'are',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'go',\n",
       " 'but',\n",
       " 'our',\n",
       " 'dessert',\n",
       " 'the',\n",
       " 'gingerbread',\n",
       " 'pear',\n",
       " 'cake',\n",
       " 'wa',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'winner',\n",
       " 'if',\n",
       " 'the',\n",
       " 'entree',\n",
       " 'were',\n",
       " 'tad',\n",
       " 'more',\n",
       " 'inspired',\n",
       " 'or',\n",
       " 'the',\n",
       " 'service',\n",
       " 'were',\n",
       " \"n't\",\n",
       " 'so',\n",
       " 'spotty',\n",
       " 'this',\n",
       " 'place',\n",
       " 'definitely',\n",
       " 'would',\n",
       " 'have',\n",
       " 'warranted',\n",
       " 'five',\n",
       " 'star',\n",
       " 'if',\n",
       " 'i',\n",
       " 'return',\n",
       " 'i',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'to',\n",
       " 'try',\n",
       " 'the',\n",
       " '75',\n",
       " 'tasting',\n",
       " 'menu',\n",
       " 'our',\n",
       " 'bill',\n",
       " 'came',\n",
       " 'out',\n",
       " 'to',\n",
       " 'about',\n",
       " '100',\n",
       " 'for',\n",
       " 'two',\n",
       " 'people',\n",
       " 'including',\n",
       " 'tip',\n",
       " 'no',\n",
       " 'drink']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_text(yelp.text[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Provide your function to `CountVectorizer` as the `analyzer` and test the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer(stop_words='english', analyzer=lemma_text, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 7878)\n",
      "('F1-score: ', 0.7584415584415584)\n",
      "('Accuracy: ', 0.9090019569471625)\n"
     ]
    }
   ],
   "source": [
    "vectorise_test(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both Accuracy and F1-score are slightly worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Build a simple TF-IDF using CountVectorizer\n",
    "\n",
    "- Term Frequency can be calulated with default CountVectorizer.\n",
    "- Inverse Document Frequency can be calculated with CountVectorizer and argument `binary=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Using TF-IDF to Summarize a Yelp Review\n",
    "\n",
    "> **Note:** Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n",
    "\n",
    "### 8.1 Build a TF-IDF predictor matrix excluding stopwords with `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_vectorize(vect):\n",
    "    X_train_vect = vect.fit_transform(X_train)\n",
    "    print(('Features: ', X_train_vect.shape[1]))\n",
    "    \n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_vect, y_train)\n",
    "    \n",
    "    y_pred = nb.predict(X_test_vect)\n",
    "    print(('F1-score: ', metrics.f1_score(y_test, y_pred)))\n",
    "    print(('Accuracy: ', metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16712)\n",
      "('F1-score: ', 0.01)\n",
      "('Accuracy: ', 0.8062622309197651)\n"
     ]
    }
   ],
   "source": [
    "# F1-score is very low!\n",
    "tf_idf_vectorize(words_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16415)\n",
      "('F1-score: ', 0.01990049751243781)\n",
      "('Accuracy: ', 0.8072407045009785)\n"
     ]
    }
   ],
   "source": [
    "# F1-score slightly improved!\n",
    "tf_idf_vectorize(tfidf_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Write a function to pull out the top 5 words by TF-IDF score from a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scores(text):\n",
    "    \n",
    "    tfidf_vectorize = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_vect = tfidf_vectorize.fit_transform(text)\n",
    "    features = tfidf_vectorize.get_feature_names_out()\n",
    "    \n",
    "    scores = pd.DataFrame(tfidf_vect.toarray(), columns=features)\n",
    "    \n",
    "    print(scores.nlargest(n=5, columns=features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            00  000  00a  00am  00pm   01   02   03  03342   04  ...  \\\n",
      "1515  0.430474  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "1783  0.371373  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "2721  0.294527  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "946   0.291961  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "2987  0.277590  0.0  0.0   0.0   0.0  0.0  0.0  0.0    0.0  0.0  ...   \n",
      "\n",
      "      zucchini  zuccini  zuchinni  zumba  zupa  zupas  zuzu  zzed  cole   m  \n",
      "1515       0.0      0.0       0.0    0.0   0.0    0.0   0.0   0.0    0.0  0.0  \n",
      "1783       0.0      0.0       0.0    0.0   0.0    0.0   0.0   0.0    0.0  0.0  \n",
      "2721       0.0      0.0       0.0    0.0   0.0    0.0   0.0   0.0    0.0  0.0  \n",
      "946        0.0      0.0       0.0    0.0   0.0    0.0   0.0   0.0    0.0  0.0  \n",
      "2987       0.0      0.0       0.0    0.0   0.0    0.0   0.0   0.0    0.0  0.0  \n",
      "\n",
      "[5 rows x 16415 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf_scores(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Extract sentiment from a review parsed with `TextBlob`\n",
    "\n",
    "Sentiment polarity ranges from -1, the most negative, to 1, the most positive. A parsed TextBlob object has sentiment which can be accessed with:\n",
    "\n",
    "    review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Calculate the sentiment for every review in the full Yelp dataset as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Create a boxplot of sentiment by star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Print reviews with the highest and lowest sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. [Bonus] Explore fun TextBlob features\n",
    "\n",
    "### 10.1 Correct spelling with `.correct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Perform spellchecking with `.spellcheck()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Extract definitions with `.define()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- NLP is a gigantic field\n",
    "- Understanding the basics broadens the types of data you can work with\n",
    "- Simple techniques go a long way\n",
    "- Use scikit-learn for NLP whenever possible"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
